<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>Love and Basketball</title>

    <!--My style changes  -->
    <link href="../static/css/my_style.css" rel="stylesheet">

    <!-- Bootstrap core CSS -->
    <link href="../static/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="../static/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../static/navbar-static-top.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="../static/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body style="font-family:'Courier';background-color:#1D2731">

    <!-- Static navbar -->
    <nav class="navbar navbar-default navbar-static-top" style="background-color:#0B3C5D">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" style="color:white"><b>Love and Basketball</b></a>
        </div>
        <div id="navbar" class="navbar-collapse collapse" style="background-color:#328CC1">
          <ul class="nav navbar-nav">
            <li><a style="color:#D9B310" href="/">Home</a></li>
            <li><a style="color:#D9B310" href="/motivation">Motivation</a></li>
            <li class="active"><a style="color:#D9B310" href="#">Model</a></li>
            <li><a style="color:#D9B310" href="/conclusions">Conclusions</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a style="color:white" href="/about">About</a></li>
            <li><a style="color:#D9B310">|</a></li>
            <li><a style="color:white" href="/contact">Contact </a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>


    <div class="container">

      <!-- Main component for a primary marketing message or call to action -->
      <div class="jumbotron">
        <p>
          <a class="btn btn-lg btn-primary" href="../../components/#navbar" role="button">View Player Positions &raquo;</a>
          <hr>
        </p>
        <a class="category" href="#unspecified">K Unspecified:<br></a>
        <a href="#affinity">Affinity Propagation</a>|<a href="#dbscan">DBSCAN</a>|<a href="#meanshift">Mean-shift</a>
        <br>
        <a class="category" href="#specified">K specified:<br></a>
        <a href="#kmeans">K-Means</a>|<a href="#gaussian">Gaussian Mixture Model</a>|<a href="#spectral">Spectral</a>|<a href="#agglomerative">Agglomerative</a>|<a href="#ward">Ward Hierarchical</a>|<a href="birch">BIRCH</a>

        <h1 class="custom"><b><u>Data</u></b></h1>
        <div>
                <img class="right" src="../static/img/bbrefsquare.png"/>
                <div class="left">
                  <p class="custom">
                    Most data was pulled from <a href="https://basketball-reference.com" target="_blank">Basketball Reference</a>, using the site's player season-finder tool.  In hopes of reducing noise, I limited my search to players who played at least 50 games, and at least 10 minutes per game, for each of the last 3 seasons.  I was hesitant to go back any further than 3 seasons, as the purpose of my study was to capture the state of the game as it exists <i>right now</i>, and the game has changed plenty in the last 5 years, let alone the last 10 or 15.  sollicitudin semper aliquet.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi lobortis ullamcorper tortor in interdum. In bibendum euismod ligula, ut tincidunt orci venenatis eget. Aenean commodo ultricies lacus, non molestie tellus pulvinar at. Vivamus fringilla elit quam, eu rutrum ex finibus non. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean tristique dolor sed tellus vehicula, ac consectetur lorem laoreet. Suspendisse potenti. Pellentesque varius fringilla metus, at ultricies purus cursus et. Integer pellentesque mi rhoncus, mattis quam ac, maximus risus. Praesent sed arcu metus. Fusce sollicitudin semper aliquet.
                    <hr>
                  </p>
                </div>
        </div>
        <div class="clear">
          <hr>
                <img class="right" src="../static/img/probb.png"/>
                <div class="left">
                  <p class="custom">
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi lobortis ullamcorper tortor in interdum. In bibendum euismod ligula, ut tincidunt orci venenatis eget. Aenean commodo ultricies lacus, non molestie tellus pulvinar at. Vivamus fringilla elit quam, eu rutrum ex finibus non. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean tristique dolor sed tellus vehicula, ac consectetur lorem laoreet. Suspendisse potenti. Pellentesque varius fringilla metus, at ultricies purus cursus et. Integer pellentesque mi rhoncus, mattis quam ac, maximus risus. Praesent sed arcu metus. Fusce sollicitudin semper aliquet.
                  </p>
                </div>
        </div>
        <div>
                <img class="left" src="../static/img/2dtsne.png"/>
                <div class="right">
                  <p class="custom">
                    t-distributed Stochastic Neighbor Embedding is a powerful manifold learning technique for visualizing higher-dimensional datasets via data embedding.  The goal here is to quickly, visually assess whether a feature space can be readily clustered, or whether, as seen here, the data seems to be evenly and randomly distributed.  This was my baseline effort, run on every feature in my dataset - I figured some feature selection was in order, and this hammered it home.  It's hard to discern any well-defined clusters, and we can expect any clustering algorithm to have a hard time with this data.
                  </p>
                </div>
        </div>
        <div class="clear">
          <hr>
                <div class="left">
                  <p class="custom">
                    With the above in mind, my immediate concern was to perform meaningful feature selection with two goals in mind:<br>
                    &emsp;a) create feature space with meaningful, separable clusters<br>
                    &emsp;b) be able to make qualitative observations on these clusters, meaningful from a basketball standpoint.
                    <hr>
                  </p>
                  <hr>
                </div>
        </div>
        <hr>
        <div>
                <img class="left" src="../static/img/2dtsne_hope.png"/>
                <div class="right">
                  <p class="custom">
                    <br><br>
                    My next effort involved simple manual feature selection - I removed features which I knew to be collinear (eg; 3P%, when 3P and 3PA are already features),features related to relatively rare events in a game (eg; PF), and features that wouldn't discriminate between clusters (eg; TOV).  Here I saw a glimmer of hope (and clusters!), but was far from satisfied.
                  </p>
                </div>
        </div>
        <br><br><br>
        <div class="clear">
                <img class="left" src="../static/img/2dtsne_new_feature_space.png"/>
                <div class="right">
                  <p class="custom">
                    <br><br>
                    I proceeded to write a script that would randomize a subset of my original feature space, run the t-SNE algorithm on the subset, run a clustering algorithm on the resulting embedding vectors, and return the resulting silhouette score.  I ran this through a loop 500 times, and wrote the feature space associated with the highest silhouette score to a list for storage and further use.  The TSNE plot seen here is the representation of this feature space.
                  </p>
                </div>
        </div>
        <div class="clear">
          <hr>
                <div class="clear">
                  <p class="custom">
                    I had yet to see how these clusters would look to basketball-fan, non-data scientist me, but these results were obviously encouraging!  The data still looked a bit "blobby", and with a couple outliers here and there, but undeniably contained well-defined, well-clumped clusters.
                    <hr>
                  </p>
                  <p class="custom">
                    This method reduced my space from 53 features originally, to a mere 13, to wit:
                  <ul>
                    <li>DRtg - Defensive Rating, an estimate of points given up per 100 possessions</li>
                    <li>eFG% - Effective Field Goal Percentage, a metric that takes into account the fact that 3-pointers are worth 150% more than 2-pointers</li>
                    <li>Pass - Total passes, per game</li>
                    <li>FT% - Free Throw Percentage</li>
                    <li>Tchs - Total touches, per game</li>
                    <li>3PA - 3 Point Attempts, per 36 minutes</li>
                    <li>STL - Steals, per 36 minutes</li>
                    <li>Dfgm - Defensive Field Goals Allowed, per game, within 6 feet of basket</li>
                    <li>3PAr - 3 Point Attempt Rate, 3 pointers attempted per field goal attempt</li>
                    <li>TRB - Total Rebounds, per 36 minutes</li>
                    <li>BLK - Blocks, per 36 minutes</li>
                    <li>OBPM - Offensive Box Plus-Minus, an ensemble metric meant to evaluate a player's entire offensive contribution</li>
                    <li>FT - Free Throws Made, per 36 minutes</li>
                  </ul>
                  </p>
                  <p class="custom">
                    It quickly occurred to me that, surely, there are many such feature spaces containing equally-well clustered data.  I ran the feature space randomizer an additional 4 times, producing the following data:<br>
                    &emsp;&emsp;&emsp;&emsp;
                    <img class="row" src="../static/img/2dtsne_feature_space_1.png">
                    <img class="row" src="../static/img/2dtsne_feature_space_2.png">
                  </p>
                  <p class="custom">
                    &emsp;&emsp;&emsp;&emsp;
                    <img class="row" src="../static/img/2dtsne_feature_space_3.png">
                    <img class="row" src="../static/img/2dtsne_feature_space_4.png">
                  </p>
                  <p class="caption">So many clusters!</p>
                  <p class="custom">
                    In light of this information, I set out to discern a pattern from these 4 spaces, in hopes of crafting the set of features that I would ultimately use in my clustering.  After much experimentation, I decided to test the viability of these feature spaces by introducing a very human condition: any model that didn't cluster Nikola Jokic's and Marc Gasol's 2016-17 seasons together would be thrown out.  I had reservations about imposing such a condition on my model - surely this flies in the exploratory nature of unsupervised learning?  Ultimately, I concluded that these algorithms needed such direction, as it would constitute evidence that the model was clustering in a meaningful way - ie; that the data from the feature space was providing a picture of a player's style, as opposed to other qualitative data about him, which I was not trying to capture.
                  </p>
                  <a name="unspecified"><h1 class="custom"><b><u>K-unspecified Clustering</u></b></h1></a>
                  <a name="affinity"><h2 class="custom"><b><i>Affinity Propagation</i></b></h2></a>
                  <p class="custom">
                    After setting this 'Jokic' condition, I set to exploring a number of algorithms that don't require a number of clusters as an input, starting with affinity propagation.  Affinity propagation functions by 'message passing' between points, in which the model iteratively selects points as 'exemplars' for other points, until determining that all points are adequately represented by their nearest exemplar.  The main limiting factor with the algorithm is its computational cost, which hardly intimidated me and my 911-record dataset.
                  </p>
                  <hr>
                  <div class="clear">
                          <img class="left" src="../static/img/affinity_snippet.png"/>
                          <div class="clear">
                            <p class="custom">
                              Iterative model analysis -
                              The extreme preference value accounts for another of affinity propagation's issues: it tends to over-cluster, so we set preference accordingly to mitigate the issue.
                            </p>
                            <p class="custom">
                              Affinity propagation performed admirably on each of the feature spaces, but with each, it highlighted certain challenges of the algorithm, and indeed, of my problem itself.  Any reduction in bias - that is to say, an increase to silhouette scores - would necessarily require a fairly intense reduction to the dimension of my feature space.  The underlying issue with this requirement was that such a drastic culling almost always resulted in clusters that were hard to explain from a basketball standpoint.  It was a constant balancing act to favor what the data said, versus what my watching experience told me.<br><br>

                              All that said, affinity propagation handled feature space 1 very well, and it appeared, to my eye, to have only a couple of issues. First, the lack of a feature that took rebounding into account; Evan Turner, for example, snuck his way into a "big man playmaker" cluster, when a rebounding feature likely would've prevented such an inclusion. The next issue was one that, ultimately, I feared would pervade any model or parameterization I settled on, namely: it had a very hard time quantifying and clustering on defensive value.  Players like Andrew Bogut were being grouped with players like Greg Monroe and Julius Randle, when the former has (in recent years) been a defensive stalwart, while the latter are comparably ineffective on the defensive end. I made a note to make a serious effort to balance offensive metrics with defensive ones when finalizing my feature space.<br><br>
                              While keeping in the back of my mind that affinity propagation could work very well if I presented it with the proper feature space, I forged on to explore other algorithms.
                            </p>
                          </div>
                  </div>
                  <hr>
                  <a name="dbscan"><h2 class="custom"><b><i>DBSCAN</i></b></h2></a>
                  <p class="custom"> DBSCAN is an algorithm built to robustly handle noise, and clusters of varying shapes.  Like affinity propagation, it carries the distinction of not needing to know how many clusters the user is looking for.  While these are desirable traits, it also carries a number of severely limiting disadvantages, like its inability to handle higher-dimensional data, as well as clusters of varying density, as it only functions on the one epsilon value provided at initiation - both of these proved to be insurmountable problems in my study.
                    <img class="left" src="../static/img/affinity_snippet.png"/>
                    <div class="clear">
                      <p class="custom">
                      More iterative clustering - The epsilon parameter defines the acceptable neighborhood size for a sample, so, not surprisingly, varying it like this wildly changes the number of clusters the algorithm will produce.  After running this on each subspace, I got a good idea of the epsilon range that would produce models containing between ~5 and ~20 clusters, for each subspace.  After doing so, I tried to optimize a bit more by comparing silhouette scores at different values of epsilon for each space - when every score came up negative, I figured I had to dig in a little more to see what was going on.<br>
                      I ran the algorithm with 'optimal' parameters on the subspace that had performed the best - fs2 - and checked the distribution of cluster labels over my data:
                    </p>
                    <div>
                            <img class="left" src="../static/img/dbscan_noise.png"/>
                            <div class="right">
                              <p class="custom">
                                <br><br>
                                Here was the culprit!  DBSCAN found 781 of my 911 records too noisy to cluster, and so labeled them -1...if this was happening over my seriously reduced subspaces, the algorithm surely wouldn't be able to handle my final feature space...
                              </p>
                            </div>
                    </div>
                    </div>
                    <a name="meanshift"><h2 class="custom"><b><i>Mean-shift</i></b></h2></a>
                    <p class="custom">Like DBSCAN, Mean-shift is a density-based algorithm that doesn't require a number of clusters be specified.
                    </p>
                    <div>
                            <img class="left" src="../static/img/meanshift_snippet.png"/><br>
                          </div>
                          <div class="clear">
                            <img class="left" src="../static/img/silhouette_error.png"/>
                            <div class="right">
                              <p class="custom">
                                Discussion about how meanshift utterly failed me :(
                              </p>
                            </div>
                          </div>
                    </div>

                    <a name="specified"><h1 class="custom">K-Specified Algorithms</h1></a>

                    <a name="kmeans"><h2 class="custom"><b><i>K-Means</i></b></h2></a>
                    <p class="custom">K-Means is a popular, very commonly-used algorithm, in part due to its ease in implementation.  The algorithm thrives in situations where clusters are spherical, cluster size is balanced, and cluster density is relatively uniform.  This was problematic, as none of those descriptors could be applied to my data.  Though the algorithm was still very much worth exploring - especially as a means to hone in on an ideal value of k for the other guided algorithms - I was not expecting any noteworthy results.
                    </p>
                    <img class="left" src="../static/img/k_optimization.png"/>
                    <div class="clear">
                      <p class="custom">
                        I used this script in an attempt to use the 'elbow method' to determine a suitable number of clusters.  The method involves visually assessing the point of diminishing return in terms of error reduction upon increasing number of clusters.  I also introduced a new metric for consideration; the Calinski-Harabasz index is based on both inter- and intra-cluster sum of squares measures.<sup> Here are the charts produced for each metric after 100 iterations:
                      </p>
                      &emsp;&nbsp;
                      <img class="row" src="../static/img/errorelbow_kmeans.png"/>
                      <img class="row" src="../static/img/silhouetteelbow_kmeans.png"/>
                      <img class="row" src="../static/img/calinskielbow_kmeans.png"/>
                      <div class="clear">
                        <p class="custom">
                          Immediately here we can see the deficiencies of K-Means show up - our error is <i>increasing</i> despite our adding more clusters, something which should not happen if the algorithm has the means to cluster the data properly.<br>
                          This is a great segue to another important concept in K-Means: PCA.  In a sense, PCA can almost be thought of as a generalized K-means algorithm - both methods seek to minimalize mean-squared reconstruction error in representing data as vectors in a smaller dimension.  The relationship between unsupervised dimension reduction and unsupervised learning isn't yet fully understood, but PCA is largely accepted as an effective "relaxation" of K-means, and thus useful as a step in preprocessing.
                          <br><br>
                          I ran the same script, but processed the data with PCA before fitting it with K-means.  Here are those results:
                        </div>
                        </p>
                        &emsp;&nbsp;
                        <img class="row" src="../static/img/errorelbow_pca.png"/>
                        <img class="row" src="../static/img/silhouetteelbow_pca.png"/>
                        <img class="row" src="../static/img/calinskielbow_pca.png"/>
                        <br>
                        <div class="clear">
                          <p class="custom">
                            While it was still clear K-means likely wouldn't get the job done, it was reassuring to see some sort of pattern emerge around the k=[10,15] range.
                          </p>
                          </div>
                    <a name="gaussian"><h2 class="custom"><b><i>Gaussian Mixture Model</i></b></h2></a>
                    <p class="custom">The Gaussian Mixture Model views the dataset as a mixture of Gaussian distributions, and can be understood as the generalized case of K-means - it's generalized in the sense that it takes into account the <i>covariance</i> of the data as well, which, functionally, means the model can take different cluster shapes into account.  Here are the elbow plots from running PCA-initialized data through GMM:
                    </p>
                      &emsp;&nbsp;
                      <img class="row" src="../static/img/error_gmm.png"/>
                      <img class="row" src="../static/img/silhouette_gmm.png"/>
                      <img class="row" src="../static/img/calinski_gmm.png"/>
                      <br>
                      <div class="clear">
                        <p class="custom">
                          These looked somewhat promising! After the artificially inflated values in k=[5,10], we see peaks in both score metrics around k=14 or k=15.  I checked to see if the other feature spaces - 1,3 and 4 - would produce similar plots:
                        </p>
                      </div>
                    &emsp;&nbsp;
                    <img class="row" src="../static/img/errorgmm_fs1.png"/>
                    <img class="row" src="../static/img/silhouettegmm_fs1.png"/>
                    <img class="row" src="../static/img/calinskigmm_fs1.png"/>
                    <br>
                    &emsp;&nbsp;
                    <img class="row" src="../static/img/errorgmm_fs3.png"/>
                    <img class="row" src="../static/img/silhouettegmm_fs3.png"/>
                    <img class="row" src="../static/img/calinskigmm_fs3.png"/>
                    <br>
                    &emsp;&nbsp;
                    <img class="row" src="../static/img/errorgmm_fs4.png"/>
                    <img class="row" src="../static/img/silhouettegmm_fs4.png"/>
                    <img class="row" src="../static/img/calinskigmm_fs4.png"/>
                    <br>
                    <div class="clear">
                      <p class="custom">
                        There were a couple takeaways here:
                        <ul>
                          <li>1. The Calinski-Harabasz scores for spaces 1 and 3 are both intriguing - they both have clear maximums, and both models appear to be clustering the way they should be.</li>

                          <li>2. All of these models are producing error plots with an elbow around k=[10,15]</li>
                        </ul>

                      </p>
                    </div>

                    <a name="spectral"><h2 class="custom"><b><i>Spectral Clustering</i></b></h2></a>
                    <p class="custom">Like DBSCAN, Mean-shift is a density-based algorithm that doesn't require a number of clusters be specified.
                    </p>
                    <a name="agglomerative"><h2 class="custom"><b><i>Agglomerative Clustering</i></b></h2></a>
                    <p class="custom">Like DBSCAN, Mean-shift is a density-based algorithm that doesn't require a number of clusters be specified.
                    </p>
                    <a name="ward"><h2 class="custom"><b><i>Ward Hierarchical</i></b></h2></a>
                    <p class="custom">Like DBSCAN, Mean-shift is a density-based algorithm that doesn't require a number of clusters be specified.
                    </p>
                    <a name="birch"><h2 class="custom"><b><i>BIRCH</i></b></h2></a>
                    <p class="custom">Like DBSCAN, Mean-shift is a density-based algorithm that doesn't require a number of clusters be specified.
                    </p>
                </div>
        </div>
        <h1 class="custom"><b><i>Regression</i></b></h1>
        <p></p>
      </div>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
